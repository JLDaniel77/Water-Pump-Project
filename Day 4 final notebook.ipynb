{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 39), (14358, 39), (59400, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "test_features = pd.read_csv('test_features.csv')\n",
    "train_features = pd.read_csv('train_features.csv')\n",
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "# assign to train, test, and submission\n",
    "X_train = train_features.drop(columns='id')\n",
    "X_test = test_features.drop(columns='id')\n",
    "y_train = train_labels.drop(columns='id')\n",
    "submission = test_features[['id']]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45042, 39), (14358, 39), (45042, 1), (14358, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split training data into train, validate. Make the validation set the same shape of the test set.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=len(X_test), stratify=y_train, random_state=42)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning, feature engineering, and categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(X):\n",
    "    # Create copy of dataframe to avoid copy warning\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Some features have missing data showing as 0 that need to be changed to nan\n",
    "    X['gps_height'] = X['gps_height'].replace(0.0, np.nan)\n",
    "    X['longitude'] = X['longitude'].replace(0.0, np.nan)\n",
    "    X['latitude'] = X['latitude'].replace(0.0, np.nan)\n",
    "    X['construction_year'] = X['construction_year'].replace(0.0, np.nan)\n",
    "    X['population'] = X['population'].replace(0.0, np.nan)\n",
    "    X['amount_tsh'] = X['amount_tsh'].replace(0.0, np.nan)\n",
    "    \n",
    "    # gps_height: replace nan values with the mean of the smallest geographical region possible\n",
    "    # Excluding subvillage due to missing values\n",
    "    X['gps_height'].fillna(X.groupby(['ward'])['gps_height'].transform('mean'), inplace=True)\n",
    "    X['gps_height'].fillna(X.groupby(['district_code'])['gps_height'].transform('mean'), inplace=True)\n",
    "    X['gps_height'].fillna(X.groupby(['region_code'])['gps_height'].transform('mean'), inplace=True)\n",
    "    X['gps_height'].fillna(X['gps_height'].mean(), inplace=True)\n",
    "    \n",
    "    # longitude: replace nan values with the mean of the smallest geographical region possible\n",
    "    # Excluding subvillage due to missing values\n",
    "    X['longitude'].fillna(X.groupby(['ward'])['longitude'].transform('mean'), inplace=True)\n",
    "    X['longitude'].fillna(X.groupby(['district_code'])['longitude'].transform('mean'), inplace=True)\n",
    "    X['longitude'].fillna(X.groupby(['region_code'])['longitude'].transform('mean'), inplace=True)\n",
    "    X['longitude'].fillna(X['longitude'].mean(), inplace=True)\n",
    "    \n",
    "    # latitude: replace nan values with the mean of the smallest geographical region possible\n",
    "    # Excluding subvillage due to missing values\n",
    "    X['latitude'].fillna(X.groupby(['ward'])['latitude'].transform('mean'), inplace=True)\n",
    "    X['latitude'].fillna(X.groupby(['district_code'])['latitude'].transform('mean'), inplace=True)\n",
    "    X['latitude'].fillna(X.groupby(['region_code'])['latitude'].transform('mean'), inplace=True)\n",
    "    X['latitude'].fillna(X['latitude'].mean(), inplace=True)\n",
    "    \n",
    "    # population: replace nan values with the mean of the smallest geographical region possible\n",
    "    # Excluding subvillage due to missing values\n",
    "    X['population'].fillna(X.groupby(['ward'])['population'].transform('median'), inplace=True)\n",
    "    X['population'].fillna(X.groupby(['district_code'])['population'].transform('median'), inplace=True)\n",
    "    X['population'].fillna(X.groupby(['region_code'])['population'].transform('median'), inplace=True)\n",
    "    X['population'].fillna(X['population'].median(), inplace=True)\n",
    "    \n",
    "    # population: replace nan values with the mean of the smallest geographical region possible\n",
    "    # Excluding subvillage due to missing values\n",
    "    X['amount_tsh'].fillna(X.groupby(['ward'])['amount_tsh'].transform('median'), inplace=True)\n",
    "    X['amount_tsh'].fillna(X.groupby(['district_code'])['amount_tsh'].transform('median'), inplace=True)\n",
    "    X['amount_tsh'].fillna(X.groupby(['region_code'])['amount_tsh'].transform('median'), inplace=True)\n",
    "    X['amount_tsh'].fillna(X['amount_tsh'].median(), inplace=True)\n",
    "    \n",
    "    # construction_year: replace nan values with the mean of the smallest geographical region possible\n",
    "    # Excluding subvillage due to missing values\n",
    "    X['construction_year'].fillna(X.groupby(['ward'])['construction_year'].transform('median'), inplace=True)\n",
    "    X['construction_year'].fillna(X.groupby(['district_code'])['construction_year'].transform('median'), inplace=True)\n",
    "    X['construction_year'].fillna(X.groupby(['region_code'])['construction_year'].transform('median'), inplace=True)\n",
    "    X['construction_year'].fillna(X['construction_year'].median(), inplace=True)\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
    "    \n",
    "    # Extract datetime data\n",
    "    X['year_recorded'] = X['date_recorded'].dt.year\n",
    "    \n",
    "    # Drop duplicate or unnecessary features\n",
    "    X = X.drop(columns=['recorded_by', 'quantity_group', 'date_recorded', 'wpt_name', 'num_private', 'subvillage',\n",
    "                       'region_code', 'management_group', 'extraction_type_group', 'extraction_type_class',\n",
    "                       'scheme_name', 'payment', 'water_quality', 'source_type', 'source_class', 'waterpoint_type_group', \n",
    "                        'public_meeting', 'permit'])\n",
    "    \n",
    "    # Several categorical features have values showing as '0'\n",
    "    # Replace '0' with nan\n",
    "    categoricals = X.select_dtypes(exclude='number').columns.tolist()\n",
    "    X[categoricals] = X[categoricals].replace('0', np.nan)\n",
    "    \n",
    "    # Convert to lowercase to collapse duplicates\n",
    "    X['waterpoint_type'] = X['waterpoint_type'].str.lower()\n",
    "    X['funder'] = X['funder'].str.lower()\n",
    "    X['basin'] = X['basin'].str.lower()\n",
    "    X['region'] = X['region'].str.lower()\n",
    "    X['source'] = X['source'].str.lower()\n",
    "    X['lga'] = X['lga'].str.lower()\n",
    "    X['management'] = X['management'].str.lower()\n",
    "    X['quantity'] = X['quantity'].str.lower()\n",
    "    X['quality_group'] = X['quality_group'].str.lower()\n",
    "    X['payment_type'] = X['payment_type'].str.lower()\n",
    "    X['extraction_type'] = X['extraction_type'].str.lower()\n",
    "    \n",
    "    # Replace nan values with 'other'\n",
    "    X[\"funder\"].fillna(\"other\", inplace=True)\n",
    "    X[\"scheme_management\"].fillna(\"other\", inplace=True)\n",
    "    X[\"installer\"].fillna(\"other\", inplace=True)\n",
    "    \n",
    "    X = X.replace(np.nan, 'other')\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Engineered features from data exploration and cleaning notebook\n",
    "    X['amount_tsh_above_1000'] = X['amount_tsh'] > 1000\n",
    "    X['water_quality_good_and_seasonal'] = (X['quality_group'] == 'good') & (X['quantity'] == 'seasonal')\n",
    "    X['water_quality_good_and_dry'] = (X['quality_group'] == 'good') & (X['quantity'] == 'dry')\n",
    "    X['water_quality_good_and_insufficient'] = (X['quality_group'] == 'good') & (X['quantity'] == 'insufficient')\n",
    "    X['water_quality_good_and_enough'] = (X['quality_group'] == 'good') & (X['quantity'] == 'enough')\n",
    "    X['water_quality_good'] = X['quality_group'] == 'good'\n",
    "    X['age'] = 2019 - X['construction_year']\n",
    "    X['years_since_inspection'] = 2019 - X['year_recorded']\n",
    "    X['x_coordinate'] = np.cos(X['latitude'] * np.cos(X['longitude']))\n",
    "    X['y_coordinate'] = np.cos(X['latitude'] * np.sin(X['longitude']))\n",
    "    X['z_coordinate'] = np.sin(X['latitude'])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(X_train, X_val, X_test):\n",
    "    X_train = X_train.copy()\n",
    "    X_val = X_val.copy()\n",
    "    X_test = X_test.copy()\n",
    "    encoder = ce.OrdinalEncoder()\n",
    "    X_train = encoder.fit_transform(X_train)\n",
    "    X_val = encoder.transform(X_val)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    \n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45042, 22), (14358, 22), (14358, 22))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run wrangle function\n",
    "X_train = wrangle(X_train)\n",
    "X_val = wrangle(X_val)\n",
    "X_test = wrangle(X_test)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45042, 33), (14358, 33), (14358, 33))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run feature_eng function\n",
    "X_train = feature_eng(X_train)\n",
    "X_val = feature_eng(X_val)\n",
    "X_test = feature_eng(X_test)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45042, 33), (14358, 33), (14358, 33))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run encode function\n",
    "X_train, X_val, X_test = encode(X_train, X_val, X_test)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_csv(y_pred, file_name):\n",
    "    \"\"\"\n",
    "    Function to create csv file to be submitted to Kaggle.com\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred: a 1-D array of model predictions\n",
    "    file_name: name for new csv file as a string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A csv file in the current directory with a column for label id and predicted labels.\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.copy()\n",
    "    submission['status_group'] = y_pred\n",
    "    submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example submission code\n",
    "# submit predictions to kaggle\n",
    "#!kaggle competitions submit -c ds3-predictive-modeling-challenge -f kaggle-submission-004.csv -m \"Kitchen sink model with hyper-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8090959743696894"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "# Set model params\n",
    "model = RandomForestClassifier(criterion='entropy', max_features=4, n_estimators=251, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Check score\n",
    "score = accuracy_score(y_val, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theta\\Anaconda3\\envs\\lambda\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for kaggle\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Make prediction csv file\n",
    "submission_csv(y_pred, 'stacking-001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to DS3 Predictive Modeling Challenge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/279k [00:00<?, ?B/s]\n",
      "  3%|2         | 8.00k/279k [00:00<00:14, 19.2kB/s]\n",
      " 32%|###1      | 88.0k/279k [00:00<00:07, 27.2kB/s]\n",
      " 40%|####      | 112k/279k [00:00<00:04, 36.3kB/s] \n",
      " 77%|#######7  | 216k/279k [00:00<00:01, 51.0kB/s]\n",
      "100%|##########| 279k/279k [00:04<00:00, 66.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c ds3-predictive-modeling-challenge -f stacking-001.csv -m \"RFC with new features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   21.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   21.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7775456191670149"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BaggingClassifier with 1000 DecisionTreeClassifier\n",
    "\n",
    "# Set model parameters\n",
    "tree = BaggingClassifier(n_estimators=1000, max_features=4, n_jobs=-3, random_state=42, verbose=10)\n",
    "\n",
    "# Fit the model\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = tree.predict(X_val)\n",
    "\n",
    "# Check score\n",
    "score = accuracy_score(y_val, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theta\\Anaconda3\\envs\\lambda\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for kaggle\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Make prediction csv file\n",
    "submission_csv(y_pred, 'stacking-002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  3.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  3.0min finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   53.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   53.0s finished\n",
      "C:\\Users\\theta\\Anaconda3\\envs\\lambda\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# BaggingClassifier with 2,000 DecisionTreeClassifier\n",
    "\n",
    "# Set model parameters\n",
    "tree = BaggingClassifier(n_estimators=2000, max_features=4, n_jobs=-3, random_state=42, verbose=10)\n",
    "\n",
    "# Fit the model\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for kaggle\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Make prediction csv file\n",
    "submission_csv(y_pred, 'stacking-003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  3.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  3.8min finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.2min finished\n",
      "C:\\Users\\theta\\Anaconda3\\envs\\lambda\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# BaggingClassifier with 3,000 DecisionTreeClassifier\n",
    "\n",
    "# Set model parameters\n",
    "tree = BaggingClassifier(n_estimators=3000, max_features=3, n_jobs=-3, random_state=42, verbose=3)\n",
    "\n",
    "# Fit the model\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for kaggle\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Make prediction csv file\n",
    "submission_csv(y_pred, 'stacking-004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  4.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  4.5min finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.2min finished\n",
      "C:\\Users\\theta\\Anaconda3\\envs\\lambda\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# BaggingClassifier with 2,000 DecisionTreeClassifier\n",
    "\n",
    "# Set model parameters\n",
    "tree = BaggingClassifier(n_estimators=2000, max_features=6, n_jobs=-3, random_state=42, verbose=10)\n",
    "\n",
    "# Fit the model\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for kaggle\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Make prediction csv file\n",
    "submission_csv(y_pred, 'stacking-005.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theta\\Anaconda3\\envs\\lambda\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier 1000 n_estimators\n",
    "\n",
    "# Set model params\n",
    "model = RandomForestClassifier(criterion='entropy', max_features=5, n_estimators=1000, n_jobs=-3, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for kaggle\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Make prediction csv file\n",
    "submission_csv(y_pred, 'stacking-006.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theta\\Anaconda3\\envs\\lambda\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier\n",
    "model = XGBClassifier(max_depth=5, n_estimators=1000, n_jobs=-3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for kaggle\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Make prediction csv file\n",
    "submission_csv(y_pred, 'stacking-007.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filenames of your submissions you want to ensemble\n",
    "files = ['stacking-001.csv', 'stacking-002.csv', 'stacking-003.csv', 'stacking-004.csv', 'stacking-005.csv',\n",
    "        'stacking-006.csv','stacking-007.csv']\n",
    "\n",
    "submissions = (pd.read_csv(file)[['status_group']] for file in files)\n",
    "ensemble = pd.concat(submissions, axis='columns')\n",
    "majority_vote = ensemble.mode(axis='columns')[0]\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = majority_vote\n",
    "submission.to_csv('my-ultimate-ensemble-submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to DS3 Predictive Modeling Challenge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/275k [00:00<?, ?B/s]\n",
      "  3%|2         | 8.00k/275k [00:00<00:11, 23.5kB/s]\n",
      " 29%|##9       | 80.0k/275k [00:00<00:06, 33.0kB/s]\n",
      " 38%|###7      | 104k/275k [00:00<00:03, 44.3kB/s] \n",
      " 46%|####6     | 128k/275k [00:00<00:02, 58.6kB/s]\n",
      "100%|##########| 275k/275k [00:05<00:00, 53.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c ds3-predictive-modeling-challenge -f my-ultimate-ensemble-submission1.csv -m \"2nd attempt at stacking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 21.2min\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribution = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(1, 10)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=XGBClassifier(n_jobs=-3, random_state=42),\n",
    "    param_distributions=param_distribution,\n",
    "    n_iter=20, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=2,\n",
    "    verbose=10,\n",
    "    return_train_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions\n",
    "best = search.best_estimator_\n",
    "y_pred = best.predict(X_test)\n",
    "\n",
    "# Create submission csv file\n",
    "submission_csv(y_pred, 'kaggle-submission-009.csv')\n",
    "\n",
    "# Submit to kaggle\n",
    "!kaggle competitions submit -c ds3-predictive-modeling-challenge -f kaggle-submission-009.csv -m \"XGB final\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lambda] *",
   "language": "python",
   "name": "conda-env-lambda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
